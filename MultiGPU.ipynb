{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MultiGPU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liuyi3013/colab/blob/main/MultiGPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULyIoqBxH0z0"
      },
      "source": [
        "# Multi-GPU Considerations\n",
        "\n",
        "These days, training neural networks with multiple GPU is often necessary.\n",
        "As SpeechBrain is strongly linked to PyTorch, we provide the same multi-GPU utilities:\n",
        "\n",
        "- *Data Parallel (DP)*\n",
        "- *Distributed Data Parallel (DDP)*.\n",
        "\n",
        "One big difference between DP and DDP is that DP can only be run on a single machine (with multiple GPUs), while  DDP can exploit GPUs across different servers as well. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giZzencLWy75"
      },
      "source": [
        "## DataParallel\n",
        "Data Parallel (DP) relies on a wrapper applied to neural network modules.\n",
        "The wrapper can be simpy applied following:\n",
        " \n",
        "`nn_modules = torch.nn.DataParallel(nn_modules).`\n",
        "\n",
        "DP uses 4 primitives to implement data parallelism:\n",
        "1. *replicate:* replicates a Module on multiple GPU devices.\n",
        "2. *scatter:* distributes the input in the first_dimension\n",
        "3. *parallel_apply:* applies a set of already-distributed inputs to a set of already-distributed models.\n",
        "4. *gather:* gathers and concatenate the outputs.\n",
        "\n",
        "\n",
        "The common pattern for using Data Parallel in SpeechBrain is the following:\n",
        "\n",
        "\n",
        "```\n",
        "cd recipes/<dataset>/<task>/\n",
        "python experiment.py params.yaml --data_parallel_backend\n",
        "```\n",
        "The selected devices can be given by setting *CUDA_VISIBLE_DEVICES=0,..,N*\n",
        "\n",
        "**IMPORTANT**: the batch size for each GPU process is: **batch_size / data_parallel_count**. So you should consider changing the batch_size value according to your need."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck1qe3KvpZPV"
      },
      "source": [
        "## Distributed Data Parallel (DDP)\n",
        "\n",
        "DDP implements data parallelism on different processes. This way, the GPUs do not necessarily have to be in the same server. This solution is much more flexible. However, the training routines must be written considering multi-threading. \n",
        "\n",
        "With SpeechBrain, we put several efforts to make sure the code is compliant with DDP. For instance, to avoid conflicts across processes we develop the `run_on_main` function. It is called when critical operations such as writing a file on disk are performed. It ensures that these operations are run in a single process only. The other processes are waiting until this operation is completed.\n",
        "\n",
        "Using DDP in speechbrain with a single server (node) is quite easy:\n",
        "\n",
        "```\n",
        "cd recipes/<dataset>/<task>/\n",
        "python -m torch.distributed.launch --nproc_per_node=4 experiment.py hyperparams.yaml --distributed_launch --distributed_backend='nccl'\n",
        "```\n",
        "\n",
        "Where:\n",
        "- nproc_per_node must be equal to the number of GPUs.\n",
        "- distributed_backend is the type of backend managing multiple processes synchronizations (e.g, 'nccl', 'gloo'). Try to switch the DDP backend if you have issues with nccl.\n",
        "\n",
        "Running DDP over multiple servers (nodes) is quite system dependent. Let's start with a simple example where a user is able to connect to each node directly. If we want to run 2 GPUs on 2 different nodes (i.e total of 4 GPUs), we must do:\n",
        "\n",
        "```shell\n",
        "# Machine 1\n",
        "cd recipes/<dataset>/<task>/\n",
        "python -m torch.distributed.launch --nproc_per_node=2 --nnodes=2 --node_rank=0 --master_addr machine_1_adress --master_port 5555 experiment.py hyperparams.yaml --distributed_launch --distributed_backend='nccl'\n",
        "\n",
        "# Machine 2\n",
        "cd recipes/<dataset>/<task>/\n",
        "python -m torch.distributed.launch --nproc_per_node=2 --nnodes=2 --node_rank=1 --master_addr machine_1_adress --master_port 5555 experiment.py hyperparams.yaml --distributed_launch --distributed_backend='nccl'\n",
        "```\n",
        "\n",
        "In this case, Machine 1 will have 2 subprocesses (subprocess1: with local_rank=0, rank=0, and subprocess2: with local_rank=1, rank=1). Machine 2 will have 2 subprocess (subprocess1: with local_rank=0, rank=2, and subprocess2: with local_rank=1, rank=3). \n",
        "\n",
        "In practice, using `torch.distributed.launch` ensures that the right environment variables are set (`local_rank` and `rank`), so you don't have to bother about it.\n",
        "\n",
        "Now, let's try to scale this up a bit with a resource manager like SLURM. Here, we will create two scripts:\n",
        "- a SBATCH script that will request the node configuration and call the second script.\n",
        "- a SRUN script that will call the training on each node.\n",
        "\n",
        "```shell\n",
        "## sbatch.sh\n",
        "\n",
        "#SBATCH --nodes=2 # We want two nodes (servers)\n",
        "#SBATCH --ntasks-per-node=1 # we will run once the next srun per node\n",
        "#SBATCH --gres=gpu:4 # we want 4 GPUs per node\n",
        "#SBATCH --job-name=SBisSOcool\n",
        "#SBATCH --cpus-per-task=10 # the only task will request 10 cores\n",
        "#SBATCH --time=20:00:00 # Everything will run for 20H.\n",
        "\n",
        "# We jump into the submission dir\n",
        "cd ${SLURM_SUBMIT_DIR}\n",
        "\n",
        "# And we call the srun that will run --ntasks-per-node times (once here) per node\n",
        "srun srun_script.sh \n",
        "```\n",
        "\n",
        "```shell\n",
        "## srun_script.sh\n",
        "\n",
        "#!/bin/bash\n",
        "\n",
        "# We jump into the submission dir\n",
        "cd ${SLURM_SUBMIT_DIR}\n",
        "\n",
        "# We activate our env\n",
        "conda activate super_cool_sb_env\n",
        "\n",
        "# We extract the master node address (the one that every node must connects to)\n",
        "LISTNODES=`scontrol show hostname $SLURM_JOB_NODELIST`\n",
        "MASTER=`echo $LISTNODES | cut -d\" \" -f1`\n",
        "\n",
        "# here --nproc_per_node=4 because we want torch.distributed to spawn 4 processes (4 GPUs). Then we give the total amount of nodes requested (--nnodes) and then --node_rank that is necessary to dissociate the node that we are calling this from.\n",
        "python -m torch.distributed.launch --nproc_per_node=4 --nnodes=${SLURM_JOB_NUM_NODES} --node_rank=${SLURM_NODEID} --master_addr=${MASTER} --master_port=5555 train.py hparams/myrecipe.yaml\n",
        "```\n",
        "\n",
        "Note that using DDP on different machines introduces a **communication overhead** that might slow down training (depending on how fast is the connection across the different machines). \n",
        "\n",
        "We would like to advise our users that despite being more efficient, DDP is also more prone to exhibit unexpected bugs. Indeed, DDP is quite server-dependent and some setups might generate errors with the PyTorch implementation of DDP.  The future version of pytorch will improve the stability of DDP.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpTa23CAD5P1"
      },
      "source": [
        "# **About SpeechBrain**\n",
        "- Website: https://speechbrain.github.io/\n",
        "- Code: https://github.com/speechbrain/speechbrain/\n",
        "- HuggingFace: https://huggingface.co/speechbrain/\n",
        "\n",
        "\n",
        "# **Citing SpeechBrain**\n",
        "Please, cite SpeechBrain if you use it for your research or business.\n",
        "\n",
        "```bibtex\n",
        "@misc{speechbrain,\n",
        "  title={{SpeechBrain}: A General-Purpose Speech Toolkit},\n",
        "  author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and Fran√ßois Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},\n",
        "  year={2021},\n",
        "  eprint={2106.04624},\n",
        "  archivePrefix={arXiv},\n",
        "  primaryClass={eess.AS},\n",
        "  note={arXiv:2106.04624}\n",
        "}\n",
        "```"
      ]
    }
  ]
}