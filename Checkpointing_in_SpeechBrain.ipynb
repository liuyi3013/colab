{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Checkpointing in SpeechBrain.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liuyi3013/colab/blob/main/Checkpointing_in_SpeechBrain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95iYRMRpskWo"
      },
      "source": [
        "# Checkpointing\n",
        "\n",
        "By *checkpointing*, we mean saving the model and all the other necessary state information (like optimizer parameters, which epoch, and which iteration), at a particular point in time. For experiments, this has two main motivations:\n",
        "- *Recovery*. Continuing an experiment from half-way through. A compute-cluster job can run out of time or memory, or there can be some simple error, which stops the experiment script before it finishes. In that case, all progress that isn't saved to disk is lost.\n",
        "- *Early stopping*. During training, performance should be monitored on a separate validation set, which gives an estimate of generalization. When training progresses, we expect validation error to decrease at first. If we train too long, though, validation error can start to increase again (due to *overfitting*). After training, we should go back to the model parameters that performed best on the validation set.\n",
        "\n",
        "Besides, it is also important to save the trained model parameters, so that the model can be used outside the experiment script.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJre_xHT7dWY"
      },
      "source": [
        "## The role of the SpeechBrain checkpointer\n",
        "\n",
        "The SpeechBrain checkpointer simply orchestrates checkpointing. It keeps track of all the things which should be included in checkpoints, how each of those is saved, where the checkpoints should go, and it centralizes loading and saving. \n",
        "\n",
        "The checkpointer doesn't actually save things to the disk itself. It either finds a suitable saving function by type (class inheritance considered), or you can provide a custom hook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RBQlaRvTSN4"
      },
      "source": [
        "## Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPX-4BBbX5L9"
      },
      "source": [
        "%%capture\n",
        "!pip install speechbrain\n",
        "import speechbrain as sb\n",
        "import torch\n",
        "from speechbrain.utils.checkpoints import Checkpointer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dau1tkXHVydl"
      },
      "source": [
        "# The SpeechBrain Checkpointer in a nutshell\n",
        "\n",
        "Run the following code block multiple times. Each time you run the block, it trains one epoch, then ends. Running the block again is similar to restarting an experiment script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBPPtyM-Wb9Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9487f156-64f2-4dcc-9895-c1e51c4a4195"
      },
      "source": [
        "# You have a model, an optimizer and an epoch counter:\n",
        "model = torch.nn.Linear(1, 1, False)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1.0)\n",
        "epoch_counter = sb.utils.epoch_loop.EpochCounter(10)\n",
        "# Create a checkpointer:\n",
        "checkpoint_dir = \"./nutshell_checkpoints\"\n",
        "checkpointer = Checkpointer(checkpoint_dir, \n",
        "                            recoverables = {\"mdl\": model,\n",
        "                                            \"opt\": optimizer, \n",
        "                                            \"epochs\": epoch_counter})\n",
        "# Now, before running the training epochs, you want to recover,\n",
        "# if that is possible (if checkpoints have already been saved.)\n",
        "# By default, the most recent checkpoint is loaded.\n",
        "checkpointer.recover_if_possible()\n",
        "# Then we run an epoch loop:\n",
        "for epoch in epoch_counter:\n",
        "    print(f\"Starting epoch {epoch}.\")\n",
        "    # Training:\n",
        "    optimizer.zero_grad()\n",
        "    prediction = model(torch.tensor([1.]))\n",
        "    loss = (prediction - torch.tensor([1.]))**2\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Model prediction={prediction.item()}, loss={loss.item()}\")\n",
        "    # And finally at the end, save an end-of-epoch checkpoint:\n",
        "    checkpointer.save_and_keep_only(meta={\"loss\":loss.item()})\n",
        "    # Now, let's \"crash\" this code block:\n",
        "    break\n",
        "else:\n",
        "    # After training (epoch loop is depleted),\n",
        "    # we want to recover the best model:\n",
        "    print(\"Epoch loop has finished.\")\n",
        "    checkpointer.recover_if_possible(min_key=\"loss\")\n",
        "    print(f\"Best model parameter: {model.weight.data}\")\n",
        "    print(f\"Achieved on epoch {epoch_counter.current}.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1.\n",
            "Model prediction=-0.08341944217681885, loss=1.1737977266311646\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et7kVaJxbt-3"
      },
      "source": [
        "# You can use this cell to reset, by deleting all checkpoints:\n",
        "checkpointer.delete_checkpoints(num_to_keep=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMgQnm44VhFH"
      },
      "source": [
        "# What does a checkpoint look like?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc8aOduf5s60"
      },
      "source": [
        "The checkpointer is given a top-level directory, where all the checkpoints go:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPt5VC3q5PCP"
      },
      "source": [
        "checkpoint_dir = \"./full_example_checkpoints\"\n",
        "checkpointer = Checkpointer(checkpoint_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7FwgfET4uCi"
      },
      "source": [
        "Each checkpoint should contain many things like model parameters and training progress."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgiLyDeT4Bw5"
      },
      "source": [
        "# You have a model, an optimizer and an epoch counter:\n",
        "model = torch.nn.Linear(1, 1, True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1.0)\n",
        "epoch_counter = sb.utils.epoch_loop.EpochCounter(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Stelxn6i4lpl"
      },
      "source": [
        " Each entity to save is assigned to the checkpointer separately, with a unique key, like a name:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai9e8ITo4oO0"
      },
      "source": [
        "checkpointer.add_recoverable(\"mdl\", model)\n",
        "checkpointer.add_recoverables({\"opt\": optimizer, \"epoch\": epoch_counter})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPWCoH6W6pK2"
      },
      "source": [
        "When a checkpoint is saved, the checkpointer creates a directory inside the top-level directory. That sub-directory represents this saved checkpoint. Inside the newly created directory each entity, that was passed to the checkpointer, gets its own file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Or0mF2xc_BRB",
        "outputId": "5666f984-eb9b-4b2a-b1d1-f1d0d9d11a7a"
      },
      "source": [
        "ckpt = checkpointer.save_checkpoint()\n",
        "print(\"The checkpoint directory was:\", ckpt.path)\n",
        "for key, filepath in ckpt.paramfiles.items():\n",
        "    print(\"The entity with key\", key, \"was saved to:\", filepath)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The checkpoint directory was: full_example_checkpoints/CKPT+2021-06-17+03-09-23+00\n",
            "The entity with key mdl was saved to: full_example_checkpoints/CKPT+2021-06-17+03-09-23+00/mdl.ckpt\n",
            "The entity with key opt was saved to: full_example_checkpoints/CKPT+2021-06-17+03-09-23+00/opt.ckpt\n",
            "The entity with key epoch was saved to: full_example_checkpoints/CKPT+2021-06-17+03-09-23+00/epoch.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3hXv3MO_8CA"
      },
      "source": [
        "## What goes in each file?\n",
        "\n",
        "That is upto the entities. The checkpointer finds a saving \"hook\" by type (class inheritance considered) and calls that hook with the object to save and a filepath.\n",
        "\n",
        "Torch entities (Module, Optimizer) have default save and load hooks already:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7TU-p2yAOgf",
        "outputId": "4b2d4146-1ee0-4beb-8c5a-fe371e405d1b"
      },
      "source": [
        "torch_hook = sb.utils.checkpoints.get_default_hook(torch.nn.Linear(1,1), sb.utils.checkpoints.DEFAULT_SAVE_HOOKS)\n",
        "print(torch_hook.__doc__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saves the obj's parameters to path.\n",
            "\n",
            "    Default save hook for torch.nn.Modules\n",
            "    For saving torch.nn.Module state_dicts.\n",
            "\n",
            "    Arguments\n",
            "    ---------\n",
            "    obj : torch.nn.Module\n",
            "        Instance to save.\n",
            "    path : str, pathlib.Path\n",
            "        Path where to save to.\n",
            "\n",
            "    Returns\n",
            "    -------\n",
            "    None\n",
            "        State dict is written to disk.\n",
            "    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVeC5y9EBB-C"
      },
      "source": [
        "Classes can register their own default saving and loading hooks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EPuL6MPBjO9",
        "outputId": "656d5f14-b6e5-4807-f6d6-fea0b0b4a61e"
      },
      "source": [
        "@sb.utils.checkpoints.register_checkpoint_hooks\n",
        "class Duck:\n",
        "    def __init__(self):\n",
        "        self.quacks = 0\n",
        "    \n",
        "    def quack(self):\n",
        "        print(\"Quack!\")\n",
        "        self.quacks += 1\n",
        "        print(f\"I have already quacked {self.quacks} times.\")\n",
        "    \n",
        "    @sb.utils.checkpoints.mark_as_saver\n",
        "    def save(self, path):\n",
        "        with open(path, \"w\") as fo:\n",
        "            fo.write(str(self.quacks))\n",
        "\n",
        "    @sb.utils.checkpoints.mark_as_loader\n",
        "    def load(self, path, end_of_epoch, device):\n",
        "        # Irrelevant for ducks:\n",
        "        del end_of_epoch\n",
        "        del device\n",
        "        with open(path) as fi:\n",
        "            self.quacks = int(fi.read())\n",
        "    \n",
        "duck = Duck()\n",
        "duckpointer = Checkpointer(\"./duckpoints\", {\"ducky\": duck})\n",
        "duckpointer.recover_if_possible()\n",
        "duck.quack()\n",
        "_ = duckpointer.save_checkpoint()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Quack!\n",
            "I have already quacked 1 times.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2aCU6GHD-Lc"
      },
      "source": [
        "## Meta info\n",
        "\n",
        "The checkpoint also stores a dictionary of meta information. You can put e.g. validation loss or some other metric there. By default, only the unix time is saved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlSrsN2hEXyB",
        "outputId": "7c58aa44-5e4b-414b-ccd1-1ed8e3349436"
      },
      "source": [
        "# Following from the cells of \"What does a checkpoint look like?\"\n",
        "checkpointer.save_checkpoint(meta={\"loss\": 15.5, \"validation-type\": \"fast\", \"num-examples\": 3})\n",
        "ckpt = checkpointer.save_checkpoint(meta={\"loss\": 14.4, \"validation-type\": \"full\"})\n",
        "print(ckpt.meta)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'unixtime': 1623899371.7361677, 'end-of-epoch': True, 'loss': 14.4, 'validation-type': 'full'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lWlW5uUFCg0"
      },
      "source": [
        "This meta information can be used to load the best checkpoint, not just the most recent one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qmdwe1SUFPAr",
        "outputId": "b4c4f3e9-9a0c-4b3f-b2a2-a3cc6d49b5fd"
      },
      "source": [
        "ckpt = checkpointer.recover_if_possible(min_key=\"loss\")\n",
        "print(ckpt.meta)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'end-of-epoch': True, 'loss': 14.4, 'unixtime': 1623899371.7361677, 'validation-type': 'full'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0SyTFCuFdgh"
      },
      "source": [
        "There are also more advanced filters available:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5qfajLQFct6",
        "outputId": "348b03bb-95d0-47d3-d51b-ce4a62a50eb3"
      },
      "source": [
        "checkpointer.save_checkpoint(meta={\"loss\": 12.1, \"validation-type\": \"fast\", \"num-examples\": 2})\n",
        "ckpt =  checkpointer.recover_if_possible(importance_key=lambda ckpt: -ckpt.meta[\"loss\"]/ckpt.meta[\"num-examples\"],\n",
        "                                 ckpt_predicate=lambda ckpt: ckpt.meta.get(\"validation-type\") == \"fast\")\n",
        "print(ckpt.meta)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'end-of-epoch': True, 'loss': 15.5, 'num-examples': 3, 'unixtime': 1623899371.732466, 'validation-type': 'fast'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cpj3kIIKHHpV"
      },
      "source": [
        "# Keeping a limited amount of checkpoints\n",
        "\n",
        "Neural models these days can be huge, and we don't need to store every checkpoint. Checkpoints can be deleted explicitly, and the same types of filters can be used as with recovery:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVRFNGbQHQy3"
      },
      "source": [
        "checkpointer.delete_checkpoints(num_to_keep=1, ckpt_predicate=lambda ckpt: \"validation-type\" not in ckpt.meta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX4oJxG9HiXr"
      },
      "source": [
        "But for convenience, there is also a method which saves and deletes at the same time:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7wOm8vKKAgr"
      },
      "source": [
        "checkpointer.save_and_keep_only(meta={\"loss\": 13.1, \"validation-type\": \"full\"},\n",
        "                                num_to_keep = 2,\n",
        "                                ckpt_predicate=lambda ckpt: ckpt.meta.get(\"validation-type\") == \"full\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paC8hwHBnjWR"
      },
      "source": [
        "# Pretraining / parameter transfer\n",
        "\n",
        "Transferring parameters from a pretrained model is different from recovery, although the have some similarities.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awsgErXzgGHi"
      },
      "source": [
        "## Finding the best checkpoint\n",
        "\n",
        "The first step in parameter transfer is to find the ideal set of parameters to take. You can use the checkpointer for that: point an empty checkpointer at the top level checkpoints directory of an experiment, and find a checkpoint with your criterion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtTuVT7pnjWT",
        "outputId": "dffce415-4071-4c1d-d1b0-b39fa272d814"
      },
      "source": [
        "\n",
        "ckpt_finder = Checkpointer(checkpoint_dir)\n",
        "best_ckpt = ckpt_finder.find_checkpoint(min_key=\"loss\",\n",
        "                                        ckpt_predicate=lambda ckpt: ckpt.meta.get(\"validation-type\") == \"full\")\n",
        "best_paramfile = best_ckpt.paramfiles[\"mdl\"]\n",
        "print(\"The best parameters were stored in:\", best_paramfile)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best parameters were stored in: full_example_checkpoints/CKPT+2021-06-17+03-09-38+00/mdl.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sllsaFKFnjWU"
      },
      "source": [
        "## Transferring parameters\n",
        "\n",
        "There is no generic formula for parameter transfer, and in a lot of cases you may have to write some custom code to connect the incoming parameters to the new model.\n",
        "\n",
        "SpeechBrain has an almost trivial implementation for transferring parameters to another torch Module, which simply loads the matching layers (by name) and ignores saved parameters for which no matching layer is found:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSdQ-9y-njWV",
        "outputId": "7d47fb41-31fd-4e08-9225-e1006b83bdaf"
      },
      "source": [
        "finetune_mdl = torch.nn.Linear(1,1,False) #This one doesn't have bias!\n",
        "with torch.no_grad():\n",
        "    print(\"Before:\", finetune_mdl(torch.tensor([1.])))\n",
        "    sb.utils.checkpoints.torch_parameter_transfer(finetune_mdl, best_paramfile, device='cpu')\n",
        "    print(\"And after:\", finetune_mdl(torch.tensor([1.])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "During parameter transfer to Linear(in_features=1, out_features=1, bias=False) loading from full_example_checkpoints/CKPT+2021-06-17+03-09-38+00/mdl.ckpt, the object could not use the parameters loaded with the key: bias\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Before: tensor([-0.6177])\n",
            "And after: tensor([0.6893])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89_vY7edkDV5"
      },
      "source": [
        "## Orchestrating transfer\n",
        "\n",
        "SpeechBrain has a parameter transfer orchestrator similar to Checkpointer: `speechbrain.utils.parameter_transfer.Pretrainer`. The point is primarily to implement the parameter download-and-load for `speechbrain.pretrained.Pretrained` subclasses such as `EncoderDecoderASR` and to aid in writing easy-to-share recipes.\n",
        "\n",
        "Similar to Checkpointer, Pretrainer handles mapping parameter files to instances, and calling the transfer code (implemented as similar hooks as checkpoint loading).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuI9bvN8CiHe"
      },
      "source": [
        "# **About SpeechBrain**\n",
        "- Website: https://speechbrain.github.io/\n",
        "- Code: https://github.com/speechbrain/speechbrain/\n",
        "- HuggingFace: https://huggingface.co/speechbrain/\n",
        "\n",
        "\n",
        "# **Citing SpeechBrain**\n",
        "Please, cite SpeechBrain if you use it for your research or business.\n",
        "\n",
        "```bibtex\n",
        "@misc{speechbrain,\n",
        "  title={{SpeechBrain}: A General-Purpose Speech Toolkit},\n",
        "  author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and Fran√ßois Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},\n",
        "  year={2021},\n",
        "  eprint={2106.04624},\n",
        "  archivePrefix={arXiv},\n",
        "  primaryClass={eess.AS},\n",
        "  note={arXiv:2106.04624}\n",
        "}\n",
        "```"
      ]
    }
  ]
}